{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Classification Example\n",
    "---\n",
    "\n",
    "This graph dataset contains the emails exchange inside an enterprise. Each node *u* represents an employee email, that is labeled by its department; each edge *(u,v)* says that *u* sent at least one email to *v*.\n",
    "\n",
    "Our objective is to predict the department in which the employee works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of some useful functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx import DiGraph\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "#Default name for embeddign file\n",
    "EMBEDDING_FILE = \"embeddings.emb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Embedding\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the path to the dataset and to its edges file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Email_dataset/\" #path to dataset\n",
    "graph_file = dataset+\"edges.ssv\" #path to edges file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.read_edgelist(graph_file, delimiter=\" \", create_using=DiGraph()) #the graph is directed, so we use DiGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, we define the Node2Vec object with the transaction probabilities that will be used at the random walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v = Node2Vec(graph, dimensions=64, walk_length=120, num_walks=200, workers=4, p=2, q=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we process the embedding using Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v_model = n2v.fit(window=10, min_count=1, batch_words=128)\n",
    "\n",
    "n2v_model.wv.save_word2vec_format(dataset+EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use the embedding generated to predict the employees departments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we saved the embedding, we can load it as a Numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.loadtxt(dataset+EMBEDDING_FILE,delimiter=' ',skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define a function to get the node embedding representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_embedded(n):\n",
    "    return vectors[n,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the data from the *labels.ssv* file and generate the dataset like with the embeddings and the department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] #data matrix initialized empty\n",
    "\n",
    "with open(\"Email_dataset/labels.ssv\") as f:\n",
    "    for line in f:\n",
    "        node,department = line.split() #get the node id and its department (class)\n",
    "        node_embedded = to_embedded(int(node)) #get the embedded representation of the node\n",
    "        data.append(np.append(node_embedded,array([department]))) #insert the embedding and the class inside the data matrix\n",
    "\n",
    "data = array(data,dtype=float) #transform the data matrix in a Numpy array\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the data and split into train and test subsets, using the *train_percentage* factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n",
    "train_percentage = 0.7\n",
    "train_size = int(len(data)*train_percentage)\n",
    "\n",
    "train_data = array(data[0:train_size])\n",
    "test_data = array(data[train_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "---\n",
    "We are going to use the KNN model to predict the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function to train the model and give the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model):\n",
    "    model.fit(train_data[:,0:-1], train_data[:,-1])\n",
    "    return model.score(test_data[:,:-1], test_data[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least, we train and compute the scores for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = train_and_eval(KNeighborsClassifier())\n",
    "print(\"KNeighborsClassifier score: {}%\".format(score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is very low. To understand this, let's analyse the departments distribution over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Email_dataset/labels.ssv\", delimiter=\" \", names=[\"Node\",\"Dep\"])\n",
    "_ = df[\"Dep\"].value_counts().plot(kind=\"bar\", figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the graph we can see that the dataset is unbalanced, so that can explain the bad accuracies that we got.\n",
    "So we are going take a subgraph that have only certain departments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the Subgraph\n",
    "---\n",
    "First, we compute a dictionary with the nodes departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {}\n",
    "with open(\"Email_dataset/labels.ssv\") as f:\n",
    "    for line in f:\n",
    "        node,department = line.split()\n",
    "        labels_dict[node] = department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, we generate a subset of edges that contains only the nodes that are from the departments defined in *labels_filter*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_filter = [\"4\",\"14\"]\n",
    "with open(\"Email_dataset/edges.ssv\",'r') as f_in:\n",
    "    with open(\"Email_dataset/edges_filtered.ssv\",'w') as f_out:\n",
    "        for line in f_in:\n",
    "            src,trg = line.split()\n",
    "            if labels_dict[src] in labels_filter and labels_dict[trg] in labels_filter:\n",
    "                f_out.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we reload the graph an compute its embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.read_edgelist(\"Email_dataset/edges_filtered.ssv\", delimiter=\" \", create_using=DiGraph())\n",
    "\n",
    "n2v = Node2Vec(graph, dimensions=128, walk_length=50, num_walks=30, workers=4, p=1, q=1)\n",
    "\n",
    "n2v_model = n2v.fit(window=50, min_count=1, batch_words=64)\n",
    "\n",
    "n2v_model.wv.save_word2vec_format(dataset+EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we process the data again, but we take only the employees that are at the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.loadtxt(dataset+EMBEDDING_FILE,delimiter=' ',skiprows=1)\n",
    "\n",
    "def to_embedded(n):\n",
    "    return vectors[n,:]\n",
    "\n",
    "data = []\n",
    "nodes = list(graph.nodes())\n",
    "with open(\"Email_dataset/labels.ssv\") as f:\n",
    "    for line in f:\n",
    "#         print(line)\n",
    "        node,department = line.split()\n",
    "        if department in labels_filter:\n",
    "            try:\n",
    "                node_embedded = to_embedded(nodes.index(node))\n",
    "                data.append(np.append(node_embedded,array([department])))\n",
    "            except Exception as e:\n",
    "#                 print(e)\n",
    "                pass\n",
    "\n",
    "data = array(data,dtype=float)\n",
    "\n",
    "np.random.shuffle(data)\n",
    "train_percentage = 0.7\n",
    "train_size = int(len(data)*train_percentage)\n",
    "\n",
    "train_data = array(data[0:train_size])\n",
    "test_data = array(data[train_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least, we run again the KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = train_and_eval(KNeighborsClassifier())\n",
    "print(\"KNeighborsClassifier score: {}%\".format(score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had an improvement at the accuracy. Let's check the accuracy with other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = train_and_eval(LogisticRegression())\n",
    "print(\"LogisticRegression score: {}%\".format(score*100))\n",
    "score = train_and_eval(SGDClassifier(max_iter=100, tol=0.001))\n",
    "print(\"SGDClassifier score: {}%\".format(score*100))\n",
    "score = train_and_eval(Perceptron(max_iter=100, tol=0.001))\n",
    "print(\"Perceptron score: {}%\".format(score*100))\n",
    "\n",
    "score = train_and_eval(SVC())\n",
    "print(\"SVC score: {}%\".format(score*100))\n",
    "\n",
    "score = train_and_eval(MLPClassifier())\n",
    "print(\"MLPClassifier score: {}%\".format(score*100))\n",
    "\n",
    "score = train_and_eval(GaussianProcessClassifier())\n",
    "print(\"GaussianProcessClassifier score: {}%\".format(score*100))\n",
    "\n",
    "score = train_and_eval(DecisionTreeClassifier())\n",
    "print(\"DecisionTreeClassifier score: {}%\".format(score*100))\n",
    "\n",
    "score = train_and_eval(BernoulliNB())\n",
    "print(\"BernoulliNB score: {}%\".format(score*100))\n",
    "score = train_and_eval(GaussianNB())\n",
    "print(\"GaussianNB score: {}%\".format(score*100))\n",
    "\n",
    "score = train_and_eval(GradientBoostingClassifier())\n",
    "print(\"GradientBoostingClassifier score: {}%\".format(score*100))\n",
    "score = train_and_eval(RandomForestClassifier())\n",
    "print(\"RandomForestClassifier score: {}%\".format(score*100))\n",
    "score = train_and_eval(ExtraTreesClassifier())\n",
    "print(\"ExtraTreesClassifier score: {}%\".format(score*100))\n",
    "score = train_and_eval(AdaBoostClassifier())\n",
    "print(\"AdaBoostClassifier score: {}%\".format(score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further tests we can change the parameters to generate the transaction probabilities, resulting in different embeddings. We let this as an excercise to the reader ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
